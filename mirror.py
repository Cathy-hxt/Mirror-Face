import os
import json
import cv2
import numpy as np
import datetime
import atexit
import time
import requests
from collections import Counter
import io
import threading
import tkinter as tk
from matplotlib import pyplot as plt
from matplotlib.patches import FancyArrowPatch
from PIL import Image, ImageDraw, ImageFont, ImageTk
from screeninfo import get_monitors
from deepface import DeepFace
from openai import OpenAI
from qianfan.resources.console.iam import IAM

# ------------------ å¸¸é‡é…ç½® ---------------------
# Face++ API é…ç½®
FACEPP_API_KEY = "qWTsVPmDQmwxnpveEbcAt0FGUklv6bBW"
FACEPP_API_SECRET = "abUpRXtULBRMT6PxkyouaO7OnEt9tP6v"

# ç™¾åº¦åƒå¸† API é…ç½®
ACCESS_KEY_ID = "ALTAKZyDAaT3UU2MudxB2u5Xzn"
ACCESS_KEY_SECRET = "7149e58678a74c87b5502f44ae667dd9"

# æ–‡ä»¶è·¯å¾„é…ç½®
EMBEDDINGS_FILE = r"E:\emotion_db\face_embeddings.npz"
IDS_FILE = r"E:\emotion_db\face_ids.json"
LOGS_FILE = r"E:\emotion_db\emotion_logs.json"
FACE_DB_PATH = r"E:\emotion_db\face_db"  # å­˜å‚¨äººè„¸å›¾ç‰‡çš„æ•°æ®åº“ç›®å½•

# ------------------ åŠ è½½æŒä¹…åŒ–æ•°æ® ---------------------
if os.path.exists(EMBEDDINGS_FILE):
    data = np.load(EMBEDDINGS_FILE)
    known_face_embeddings = [data[key] for key in data.files]
else:
    known_face_embeddings = []

if os.path.exists(IDS_FILE):
    with open(IDS_FILE, 'r', encoding='utf-8') as f:
        known_face_ids = json.load(f)
else:
    known_face_ids = []

if os.path.exists(LOGS_FILE):
    with open(LOGS_FILE, 'r', encoding='utf-8') as f:
        emotion_logs = json.load(f)
else:
    emotion_logs = {}

# ------------------ ä¿å­˜æ•°æ® ---------------------
def save_state():
    """ä¿å­˜å½“å‰çš„æƒ…ç»ªè¯†åˆ«æ•°æ®ã€IDå’Œæ—¥å¿—åˆ°æ–‡ä»¶ã€‚"""
    if known_face_embeddings:
        np.savez(EMBEDDINGS_FILE, *known_face_embeddings)
    with open(IDS_FILE, 'w', encoding='utf-8') as f:
        json.dump(known_face_ids, f, ensure_ascii=False, indent=4)
    with open(LOGS_FILE, 'w', encoding='utf-8') as f:
        json.dump(emotion_logs, f, ensure_ascii=False, indent=4)

atexit.register(save_state)

# ------------------ Face++ API äººè„¸æ£€æµ‹ ---------------------
def facepp_detect(face_img):
    """ä½¿ç”¨Face++ APIè¿›è¡Œäººè„¸æ£€æµ‹å¹¶è¿”å›æƒ…ç»ªã€æ€§åˆ«ã€å¹´é¾„ç­‰å±æ€§ã€‚"""
    url = "https://api-cn.faceplusplus.com/facepp/v3/detect"
    ret, buf = cv2.imencode('.jpg', face_img)
    if not ret:
        print("å›¾åƒç¼–ç å¤±è´¥")
        return None
    files = {
        "image_file": ("face.jpg", buf.tobytes(), "image/jpeg")
    }
    params = {
        "api_key": FACEPP_API_KEY,
        "api_secret": FACEPP_API_SECRET,
        "return_attributes": "emotion,gender,age,beauty",
        "return_landmark": 0  # ä¸è¿”å›é¢éƒ¨å…³é”®ç‚¹
    }
    
    try:
        response = requests.post(url, files=files, params=params)
        result = response.json()
        return result
    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥ï¼š{str(e)}")
        return None

# ------------------ OpenCV äººè„¸æ£€æµ‹ ---------------------
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# ------------------ AI å¯¹è¯ API é…ç½® ---------------------
client = OpenAI(api_key="YOUR_API_KEY", base_url="https://api.openai.com")

# ------------------ æƒ…ç»ªè¡¨æƒ…æ˜ å°„ ---------------------
emotion_emoji = {
    "happiness": "ğŸ˜€", "neutral": "ğŸ˜", "sadness": "ğŸ˜", "anger": "ğŸ˜¡", "fear": "ğŸ˜¨",
    "disgust": "ğŸ˜·", "surprise": "ğŸ˜±", "unknown": "â“", "": ""
}

# ------------------ AI å¯¹è¯ç”Ÿæˆ ---------------------
def get_chat_response(name, emotion, gender, age, beauty_score):
    """æ ¹æ®ç”¨æˆ·çš„æƒ…ç»ªä¿¡æ¯ç”Ÿæˆå¯¹è¯å†…å®¹ã€‚"""
    prompt = f"User {name} is feeling {emotion}, gender: {gender}, age: {age} years, beauty score: {beauty_score} out of 100. Please generate a natural conversation to express understanding or comfort."
    completion = client.chat.completions.create(
        model="ernie-3.5-8k",
        messages=[{'role': 'user', 'content': prompt}]
    )
    return completion.choices[0].message.content.strip()

# ------------------ æƒ…ç»ªæ—¶é’Ÿç”Ÿæˆ ---------------------
def get_emotion_clock_img(person_id):
    """ä¸ºæŒ‡å®šçš„ç”¨æˆ·ç”Ÿæˆæƒ…ç»ªæ—¶é’Ÿå›¾åƒã€‚"""
    today = datetime.datetime.now().date().isoformat()
    person_logs = [log for log in emotion_logs.get(person_id, []) if log['timestamp'][:10] == today]
    hour_emotion = [""] * 12  # 7 AM åˆ° 6 PMï¼Œå…±12å°æ—¶
    hours = list(range(7, 19))

    for log in person_logs:
        timestamp = datetime.datetime.fromisoformat(log['timestamp'])
        best_hour = min(hours, key=lambda hour: abs((timestamp - timestamp.replace(hour=hour, minute=0)).total_seconds()))
        hour_emotion[best_hour - 7] = log['emotion']

    emotions = [emotion_emoji.get(e, "") for e in hour_emotion]
    fig, ax = plt.subplots(figsize=(8, 8), dpi=100)
    ax.set_facecolor('black')
    ax.axis('off')
    ax.set_aspect('equal')
    n = 12
    radius = 1.0
    d = 0.2
    theta0 = -2 * np.pi / 3  # -120åº¦
    for i in range(n):
        theta = theta0 - i * np.pi / 6
        x0 = radius * np.cos(theta)
        y0 = radius * np.sin(theta)
        ax.text(x0, y0, emotions[i], fontsize=40, ha='center', va='center', color='white')
        ax.text(x0, y0 + d, str(hours[i]), fontsize=14, ha='center', va='bottom', color='white')

    # æ—¶é’ˆæŒ‡å‘å½“å‰å°æ—¶
    now = datetime.datetime.now()
    hour = now.hour
    if now.minute >= 30:
        hour += 1
    hour_idx = hour - 7
    if hour_idx < 0:
        hour_idx = 0
    elif hour_idx >= n:
        hour_idx = n - 1
    hour_hand_angle = theta0 - hour_idx * np.pi / 6

    x_start, y_start = 0, 0
    x_end = 0.8 * np.cos(hour_hand_angle)
    y_end = 0.8 * np.sin(hour_hand_angle)

    arrow = FancyArrowPatch(
        (x_start, y_start), (x_end, y_end),
        arrowstyle='-|>,head_width=0.15,head_length=0.25',
        color='#FF3366',
        linewidth=4,
        alpha=0.85,
        mutation_scale=30
    )
    ax.add_patch(arrow)

    plt.xlim(-1.3, 1.3)
    plt.ylim(-1.3, 1.3)
    ax.set_title(f'{person_id} Emotion Clock', fontsize=18, color='white', pad=20)
    buf = io.BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0) 

    plt.close(fig)
    buf.seek(0)
    img_pil = Image.open(buf).convert('RGB')
    img_np = np.array(img_pil)
    img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
    # ä¿å­˜åˆ°å½“å‰ç›®å½•
    output_path = "E:\output.png"
    cv2.imwrite(output_path, img_cv)
    print(f"æƒ…ç»ªæ—¶é’Ÿå·²ä¿å­˜åˆ° {output_path}")
    return img_cv

# ------------------ è¿è¡Œæ˜¾ç¤ºæƒ…ç»ªä¸å¯¹è¯ ---------------------
def show_dialog_clock_emotion(dialog_text, clock_img, most_common_emotion):
    """åœ¨å…¨å±æ˜¾ç¤ºæƒ…ç»ªè¡¨æƒ…å’Œå¯¹è¯å†…å®¹ã€‚"""
    monitor = get_monitors()[1]
    screen_w, screen_h = monitor.width, monitor.height

    root = tk.Tk()
    root.overrideredirect(True)
    root.geometry(f"{screen_w}x{screen_h}+{monitor.x}+{monitor.y}")
    canvas = tk.Canvas(root, width=screen_w, height=screen_h, bg='black', highlightthickness=0)
    canvas.pack(fill='both', expand=True)

    def show_emoji():
        canvas.delete("all")
        img = Image.new('RGB', (screen_w, screen_h), color='black')
        draw = ImageDraw.Draw(img)
        try:
            font_emoji = ImageFont.truetype("seguiemj.ttf", int(screen_h * 0.3))
        except:
            font_emoji = ImageFont.load_default()
        emoji = emotion_emoji.get(most_common_emotion, "")
        bbox = draw.textbbox((0, 0), emoji, font=font_emoji)
        w_emoji = bbox[2] - bbox[0]
        h_emoji = bbox[3] - bbox[1]
        draw.text(((screen_w-w_emoji)//2, int((screen_h-h_emoji)*0.5)), emoji, font=font_emoji, fill=(255,255,255))
        tk_img = ImageTk.PhotoImage(img)
        canvas.create_image(0, 0, anchor='nw', image=tk_img)
        canvas.image = tk_img
        root.after(2000, show_text)

    def show_text():
        canvas.delete("all")
        try:
            font_text = ImageFont.truetype("simhei.ttf", int(screen_h * 0.04))
        except:
            font_text = ImageFont.load_default()
        text_lines = textwrap.wrap(dialog_text, width=30)
        current_line_idx = 0
        current_char_idx = 0

        def update_text():
            nonlocal current_line_idx, current_char_idx
            canvas.delete("all")
            img = Image.new('RGB', (screen_w, screen_h), color='black')
            draw = ImageDraw.Draw(img)
            # å·²å®Œå…¨æ˜¾ç¤ºçš„æ•´è¡Œ
            for idx in range(current_line_idx):
                line = text_lines[idx]
                bbox = draw.textbbox((0, 0), line, font=font_text)
                w_line = bbox[2] - bbox[0]
                h_line = bbox[3] - bbox[1]
                y = int(screen_h * 0.5) + idx * h_line
                draw.text(((screen_w - w_line) // 2, y), line, font=font_text, fill=(255,255,255))
            # å½“å‰è¡Œçš„éƒ¨åˆ†æ–‡æœ¬
            if current_line_idx < len(text_lines):
                line = text_lines[current_line_idx]
                partial = line[:current_char_idx+1]
                bbox = draw.textbbox((0, 0), partial, font=font_text)
                w_line = bbox[2] - bbox[0]
                h_line = bbox[3] - bbox[1]
                y = int(screen_h * 0.5) + current_line_idx * h_line
                draw.text(((screen_w - w_line) // 2, y), partial, font=font_text, fill=(255,255,255))
            # æ›´æ–°ç”»å¸ƒ
            tk_img = ImageTk.PhotoImage(img)
            canvas.create_image(0, 0, anchor='nw', image=tk_img)
            canvas.image = tk_img
            # æ›´æ–°ç´¢å¼•å¹¶è®¾å®šä¸‹ä¸€æ¬¡å»¶è¿Ÿ
            if current_line_idx < len(text_lines):
                current_char_idx += 1
                if current_char_idx >= len(text_lines[current_line_idx]):
                    current_char_idx = 0
                    current_line_idx += 1
                    delay = 500   # è¡Œå°¾ç¨ä½œåœé¡¿
                else:
                    delay = 100
                root.after(delay, update_text)
            else:
                root.after(2000, show_clock)

        update_text()
    def show_clock():
        canvas.delete("all")
        if clock_img is None:
            root.after(2000, clear_screen)
            return

        # å‡†å¤‡åŸå§‹ PIL å›¾åƒ
        if isinstance(clock_img, np.ndarray):
            clock_pil = Image.fromarray(clock_img)
        else:
            clock_pil = clock_img

        # è®¡ç®—æœ€ç»ˆæ˜¾ç¤ºå¤§å°ï¼Œä¿æŒæ¯”ä¾‹ï¼Œä¸è¶…è¿‡ 60% å®½åº¦æˆ– 60% é«˜åº¦
        max_w = int(screen_w * 0.5)
        max_h = int(screen_h * 0.5)
        orig_w, orig_h = clock_pil.size
        base_scale = max(max_w / orig_w, max_h / orig_h, 1.0)
        target_w = int(orig_w * base_scale)
        target_h = int(orig_h * base_scale)

        steps = 20         # åŠ¨ç”»å¸§æ•°
        interval = 50      # æ¯å¸§å»¶è¿Ÿ (ms)

        def animate(step):
            canvas.delete("all")
            factor = (step + 1) / steps
            w = int(target_w * factor)
            h = int(target_h * factor)
            frame = clock_pil.resize((w, h), Image.LANCZOS)

            img = Image.new('RGB', (screen_w, screen_h), color='black')
            x = (screen_w - w) // 2
            y = (screen_h - h) // 2
            img.paste(frame, (x, y))

            tk_img = ImageTk.PhotoImage(img)
            canvas.create_image(0, 0, anchor='nw', image=tk_img)
            canvas.image = tk_img

            if step + 1 < steps:
                root.after(interval, lambda: animate(step + 1))
            else:
                root.after(2000, clear_screen)

        animate(0)
    def clear_screen():
        canvas.delete("all")
        canvas.configure(bg='black')
        root.after(1000, root.destroy)  # 1ç§’åå…³é—­çª—å£

    root.after(500, show_emoji)  # 0.5ç§’åå¼€å§‹
    root.mainloop()

# ------------------ å¯åŠ¨é»‘å± ---------------------
def init_black_screen():
    """åˆå§‹åŒ–å¹¶å¯åŠ¨é»‘å±çª—å£ã€‚"""
    global black_root, black_canvas
    monitor = get_monitors()[1]
    black_root = tk.Tk()
    black_root.overrideredirect(True)
    black_root.geometry(f"{monitor.width}x{monitor.height}+{monitor.x}+{monitor.y}")
    black_canvas = tk.Canvas(black_root,
                             width=monitor.width,
                             height=monitor.height,
                             bg='black',
                             highlightthickness=0)
    black_canvas.pack(fill='both', expand=True)
    # å¼€å¯åå°çº¿ç¨‹è¿è¡Œä¸»å¾ªç¯ï¼Œä¸é˜»å¡åç»­é€»è¾‘
    threading.Thread(target=black_root.mainloop, daemon=True).start()

# å¯åŠ¨é»‘å±
init_black_screen()

# ------------------ æ‰“å¼€æ‘„åƒå¤´ ---------------------
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    raise RuntimeError("æ— æ³•æ‰“å¼€æ‘„åƒå¤´")

# ------------------ åˆå§‹åŒ–çŠ¶æ€ ---------------------
face_present = False
face_start_time = None
face_end_time = None
recognizing = False
main_emotion_displayed = False
dialog_displaying = False
dialog_text = ""
dialog_start_time = None
clock_img = None
most_common_emotion = None
destroyWindow = False
black_root = None

# ------------------ äººè„¸æ£€æµ‹å’Œè¯†åˆ«ä¸»å¾ªç¯ ---------------------
while True:
    ret, frame = cap.read()
    if not ret:
        break

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    if len(faces) > 0:
        if not face_present:
            face_start_time = time.time()
            face_present = True
            main_emotion_displayed = False
            dialog_displaying = False
            dialog_text = ""
            dialog_start_time = None
            clock_img = None
        else:
            # äººè„¸æŒç»­å‡ºç°è¶…è¿‡2ç§’å¹¶ä¸”æœªå¼€å§‹è¯†åˆ«ï¼Œä¸”æœªæ˜¾ç¤ºä¸»æƒ…ç»ª
            if not recognizing and not main_emotion_displayed and time.time() - face_start_time >= 2:
                recognizing = True  # æ»¡2ç§’ï¼Œå¼€å§‹è¯†åˆ«
                session_emotions = []  # åˆå§‹åŒ–æƒ…ç»ªç»Ÿè®¡åˆ—è¡¨ï¼ˆäººè„¸æ­£å¯¹æ‘„åƒå¤´æœŸé—´ï¼‰
                gender = "unknown"
                age = "unknown"
                beauty_score = "unknown"
                session_start_time = None
                session_person_id = None
            # æ»¡6ç§’ï¼Œç»Ÿè®¡ä¸»æƒ…ç»ªå¹¶æ˜¾ç¤ºå¯¹è¯å’Œæ—¶é’Ÿ
            if recognizing and not main_emotion_displayed and time.time() - face_start_time >= 6:
                recognizing = False # äººè„¸æ­£å¯¹æ‘„åƒå¤´è¶…è¿‡6ç§’,ç»“æŸè¯†åˆ«
                if session_emotions and session_person_id:
                    most_common_emotion = Counter(session_emotions).most_common(1)[0][0]
                    if session_person_id not in emotion_logs:
                        emotion_logs[session_person_id] = []
                    emotion_logs[session_person_id].append({
                        'timestamp': session_start_time,
                        'emotion': most_common_emotion,
                        'gender': gender,
                        'age': age,
                        'beauty_score': beauty_score
                    })
                    save_state()
                    # è·å–ä»Šå¤©çš„å†å²æƒ…ç»ª
                    today = datetime.datetime.now().date().isoformat()
                    today_logs = [
                        log['emotion']
                        for log in emotion_logs[session_person_id]
                        if log['timestamp'][:10] == today
                    ]    
                    # è°ƒç”¨AIå¯¹è¯
                    conversation = get_chat_response(session_person_id, most_common_emotion, gender, age, beauty_score)
                    dialog_displaying = True
                    dialog_text = f"{conversation}"
                    dialog_start_time = time.time()                    
                    # ç»˜åˆ¶æ—¶é’Ÿæ ·å¼
                    clock_img = get_emotion_clock_img(session_person_id)
                    main_emotion_displayed = True  # é˜²æ­¢é‡å¤æ˜¾ç¤º
        face_end_time = None  # é‡ç½®ç¦»å¼€æ—¶é—´
    else:
        if face_present:
            if face_end_time is None:
                face_end_time = time.time()
            elif time.time() - face_end_time >= 3: # äººè„¸ç¦»å¼€3ç§’ï¼Œç»“æŸè¯†åˆ«
                face_present = False
                recognizing = False  # äººè„¸ç¦»å¼€åï¼Œç»“æŸè¯†åˆ«
                face_start_time = None
                face_end_time = None
                main_emotion_displayed = False  # é‡ç½®ä¸»æƒ…ç»ªæ˜¾ç¤ºçŠ¶æ€
                dialog_displaying = False
                dialog_text = ""
                dialog_start_time = None
                clock_img = None
                destroyWindow = True  # é‡ç½®å¯¹è¯çª—å£çŠ¶æ€
                most_common_emotion = None

    # åªæœ‰åœ¨recognizingä¸ºTrueæ—¶æ‰è¿›è¡Œè¯†åˆ«
    if recognizing and len(faces) > 0:
        for (x, y, w, h) in faces:
            face_img = rgb_frame[y:y+h, x:x+w]

            # ç”¨ DeepFace.find åœ¨åº“é‡ŒæŸ¥æ‰¾ç›¸ä¼¼è„¸
            df_list = DeepFace.find(img_path=face_img, db_path=FACE_DB_PATH, 
                               model_name='Facenet', enforce_detection=False)
            df = df_list[0]  # åªå–ç¬¬ä¸€ä¸ªç»“æœ
            if df.empty:
                # åº“ä¸­æ— åŒ¹é…ï¼Œè§†ä¸ºæ–°é¢å­”
                person_name = input("æ£€æµ‹åˆ°æ–°äººï¼Œè¯·è¾“å…¥åå­—ï¼š").strip()
                # å¦‚æœè¯¯è¯†åˆ«ä¸ºæ–°äººï¼ŒæŒ‰å›è½¦é”®è·³è¿‡
                if not person_name:
                    print("å·²è·³è¿‡æœ¬æ¬¡æœªè¯†åˆ«çš„äººè„¸ã€‚")
                    continue  # è·³è¿‡æœ¬æ¬¡å¾ªç¯

                # æ–°å»ºæ–°åå­—çš„æ–‡ä»¶å¤¹
                person_dir = os.path.join(FACE_DB_PATH, person_name)
                os.makedirs(person_dir, exist_ok=True)

                # é‡‡é›†å¤šå¼ å›¾ç‰‡
                num_samples = 3  # ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´é‡‡é›†æ•°é‡
                for i in range(num_samples):
                    # é‡æ–°è¯»å–ä¸€å¸§
                    ret, sample_frame = cap.read()
                    if not ret:
                        continue
                    rgb_sample = cv2.cvtColor(sample_frame, cv2.COLOR_BGR2RGB)
                    sample_face = rgb_sample[y:y+h, x:x+w]
                    save_path = os.path.join(person_dir, f"{person_name}_{i+1}.jpg")
                    cv2.imwrite(save_path, cv2.cvtColor(sample_face, cv2.COLOR_RGB2BGR))
                    # åœ¨çª—å£ä¸Šæ˜¾ç¤ºé‡‡é›†è¿›åº¦
                    cv2.putText(
                        sample_frame,f"é‡‡é›†ä¸­: {i+1}/{num_samples}",(30, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 0, 255),2
                    )
                    cv2.imshow('é‡‡é›†ä¸­', sample_frame)
                    cv2.waitKey(500)  # æ¯éš”0.5ç§’é‡‡ä¸€å¼ 

                emotion_logs[person_name] = []
                save_state()
                person_id = person_name
            else:
                # æœ‰åŒ¹é…ï¼Œé€‰å–ç›¸ä¼¼è·ç¦»æœ€è¿‘çš„
                best_match = df.iloc[df['distance'].idxmin()]
                matched_img_path = best_match['identity']  # identityå­—æ®µæ˜¯å›¾ç‰‡ç»å¯¹è·¯å¾„
                person_name = os.path.basename(os.path.dirname(matched_img_path))
                person_id = person_name

            # æƒ…ç»ªè¯†åˆ«
            result = facepp_detect(face_img) # é€šè¿‡Face++ APIè¿›è¡Œæƒ…ç»ªè¯†åˆ«
            if result and "faces" in result and result["face_num"] > 0:
                face = result["faces"][0]
                attributes = face["attributes"]
                # æå–æƒ…ç»ªã€æ€§åˆ«ã€å¹´é¾„å’Œé¢œå€¼åˆ†æ•°
                if "emotion" in attributes:
                    emotions = attributes["emotion"]
                    # å–æƒ…ç»ªåˆ†æ•°æœ€é«˜çš„æƒ…ç»ª
                    dominant_emotion = max(emotions, key=emotions.get)
                else:
                    dominant_emotion = "unknown" 
                if "gender" in attributes:
                    gender = attributes["gender"]["value"]
                if "age" in attributes:
                    age = attributes["age"]["value"]
                if "beauty" in attributes:
                    beauty_score = attributes["beauty"]["female_score"]
            ts = datetime.datetime.now().isoformat()
            if session_start_time is None:
                session_start_time = ts
                session_person_id = person_id
            session_emotions.append(dominant_emotion)
            if person_id not in emotion_logs:
                emotion_logs[person_id] = []

            # è§†é¢‘æ˜¾ç¤º
            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
            cv2.putText(frame, f"{person_id}: {dominant_emotion}",
                        (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            
    # æŒç»­æ˜¾ç¤ºå¯¹è¯å†…å®¹ï¼Œç›´åˆ°äººè„¸ç¦»å¼€
    if dialog_displaying and dialog_text:
        show_dialog_clock_emotion(dialog_text, clock_img, most_common_emotion)
    if destroyWindow:
        cv2.destroyWindow('Dialog & Clock')
 
    cv2.imshow('Video Feed', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
save_state()
print("ç¨‹åºç»“æŸï¼Œå·²ä¿å­˜æ‰€æœ‰æ•°æ®")
